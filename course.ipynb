{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3950bea4",
   "metadata": {},
   "source": [
    "# Cogito Computer Vision Course\n",
    "\n",
    "Welcome to the Cogito Computer Vision Course!\n",
    "In this course you will learn about the fundamentals of computer vision, including image processing, feature extraction, and object classification.\n",
    "An additional resource for learning about convolutional neural networks (CNNs) can be found in [this link](https://poloclub.github.io/cnn-explainer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3751dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brage/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31afdb5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "dataset = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570eecd",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "A dataset of fashion images, to practice computer vision. The dataset contains 60,000 training images and 10,000 test images of clothing items, such as shirts, shoes, and bags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.classes\n",
    "print(\"The class names are: \", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd7071",
   "metadata": {},
   "source": [
    "Lets look at a picture from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image, first_label = dataset[0]\n",
    "img = first_image.numpy().transpose((1, 2, 0))\n",
    "print(f\"This is a {class_names[first_label]}\")\n",
    "print(f\"The shape of the image is {img.shape}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c023fe9",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "We can also apply max pooling to the image. Max pooling is a downsampling technique that reduces the spatial dimensions of the image, while retaining the most important features. Here, we use a kernel size of 2 and a stride of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset[0][0].unsqueeze(0)  # Add batch dimension\n",
    "pool = torch.nn.MaxPool2d(kernel_size=2, stride=4)\n",
    "pooled_img = pool(img)\n",
    "plt.imshow(pooled_img.squeeze(0).permute(1, 2, 0).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804fa64",
   "metadata": {},
   "source": [
    "This can be done multiple times. You can see how the image gets smaller and smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dobbel_pooled = pool(pooled_img)\n",
    "plt.imshow(dobbel_pooled.squeeze(0).permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222f95c",
   "metadata": {},
   "source": [
    "# CNN Filters\n",
    "Lets look at the filters in a convolutional neural network. Here is an example picture of a cat: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d697105",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cdn.britannica.com/70/234870-050-D4D024BB/Orange-colored-cat-yawns-displaying-teeth.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://cdn.britannica.com/70/234870-050-D4D024BB/Orange-colored-cat-yawns-displaying-teeth.jpg\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "image = plt.imread(BytesIO(response.content), format=\"jpg\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e5f52",
   "metadata": {},
   "source": [
    "If we apply a filter to the image, we can see how it highlights certain features. For example, a filter that detects edges will highlight the edges in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = torch.tensor([[[[-1, -1, -1],\n",
    "                         [-1,  8, -1],\n",
    "                         [-1, -1, -1]]]], dtype=torch.float32)  # Edge detection filter\n",
    "conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
    "relu_layer = torch.nn.ReLU()\n",
    "conv_layer.weight = torch.nn.Parameter(filter.repeat(1, 3, 1, 1))  # Repeat filter for 3 input channels\n",
    "image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()  # Add batch dimension and convert to float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b451e",
   "metadata": {},
   "source": [
    "Without Relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_image = conv_layer(image_tensor)\n",
    "plt.imshow(filtered_image.squeeze(0).permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bd513",
   "metadata": {},
   "source": [
    "With Relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_image = conv_layer(image_tensor)\n",
    "filtered_image = relu_layer(filtered_image)\n",
    "plt.imshow(filtered_image.squeeze(0).permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6bc8a",
   "metadata": {},
   "source": [
    "# Task 1 - Experiment with Filters\n",
    "We can see how another filter might affect the image. Try to change the filter values and see how it affects the image.\n",
    "\n",
    "Here are some examples of filters:\n",
    "![image.png](https://miro.medium.com/v2/resize:fit:1400/1*UaO9cemImbhwMVQOoUTPLQ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c003662",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = torch.tensor([[[[ 0, 1, 0],\n",
    "                         [-1, 1, 1],\n",
    "                         [ 0, 0, 1]]]], dtype=torch.float32)  # Another filter\n",
    "conv_layer.weight = torch.nn.Parameter(filter.repeat(1, 3, 1, 1))  # Repeat filter for 3 input channels\n",
    "filtered_image = conv_layer(image_tensor)\n",
    "filtered_image = relu_layer(filtered_image)\n",
    "plt.imshow(filtered_image.squeeze(0).permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2be80",
   "metadata": {},
   "source": [
    "# Task 2 - Explore Data Augmentation\n",
    "\n",
    "Uncomment a transformation and run the cell to see what it does. You can experiment with the parameter values too, if you like. (The factor parameters should be greater than 0 and, generally, less than 1.) Run the cell again if you'd like to get a new random image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomVerticalFlip, RandomRotation, ColorJitter, GaussianBlur, RandomResizedCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to change some of the parameters below and see how the image changes. Also try to add/remove transforms.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "    RandomRotation(degrees=30),\n",
    "    RandomResizedCrop(size=128, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "dataset = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029a70c",
   "metadata": {},
   "source": [
    "# Create a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ac4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device) # The device should be CUDA\n",
    "\n",
    "class_names = dataset.classes\n",
    "# Create a model\n",
    "model = Sequential(\n",
    "    Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "    MaxPool2d(kernel_size=2, stride=2),\n",
    "    Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    MaxPool2d(kernel_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    Linear(in_features=784, out_features=128),\n",
    "    Linear(in_features=128, out_features=10)\n",
    ").to(device)\n",
    "# Test the model with the first image\n",
    "img = dataset[0][0].unsqueeze(0).to(device)  # Add batch dimension\n",
    "output = model(img)\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "print(f\"The model predicts this image as a {class_names[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c140942",
   "metadata": {},
   "source": [
    "This model is not trained, so the output will be random. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b4eb4",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "Below is an example of how to train a model on the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6019c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(root='data/FashionMNIST', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='data/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 1\n",
    "batch_losses = []  # List to store loss for each batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Training on batch number: {i}\")\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store batch loss\n",
    "        batch_losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot the loss over batches\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(batch_losses, label='Batch Loss')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Batch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b171bb",
   "metadata": {},
   "source": [
    "We can then evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812982f",
   "metadata": {},
   "source": [
    "# Task 3 - Design your own model\n",
    "Now you can try to create your own model and train it on the Fashion MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3af7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    # Design your own model\n",
    ").to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # Add your data augmentations here\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# You may change these values\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ec2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
