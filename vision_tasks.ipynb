{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340bf2f8",
   "metadata": {},
   "source": [
    "# Task 0: Try out some of the pretrained models\n",
    "This notebook will guide you through some of the most common computer vision tasks, including object detection, image segmentation, and pose estimation. We will use the [Ultralytics YOLO](https://docs.ultralytics.com/) library, which provides a simple and efficient way to perform these tasks using pretrained models.\n",
    "\n",
    "YOLO (You Only Look Once) is a family of real-time object detection models that are known for their speed and accuracy. The Ultralytics YOLO library provides a simple interface to use these models for various computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3451b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python-headless ultralytics requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86d29a",
   "metadata": {},
   "source": [
    "## Object Bounding Box Detection\n",
    "Oriented object detection goes a step further than standard object detection by introducing an extra angle to locate objects more accurately in an image.\n",
    "\n",
    "The output of an oriented object detector is a set of rotated bounding boxes that precisely enclose the objects in the image, along with class labels and confidence scores for each box. Oriented bounding boxes are particularly useful when objects appear at various angles, such as in aerial imagery, where traditional axis-aligned bounding boxes may include unnecessary background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f182dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math \n",
    "# start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# model\n",
    "model = YOLO(\"yolo-Weights/yolov8n.pt\")\n",
    "\n",
    "# object classes\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # coordinates\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "\n",
    "            # class name\n",
    "            cls = int(box.cls[0])\n",
    "\n",
    "            # object details\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1adde",
   "metadata": {},
   "source": [
    "## Image Segmentation\n",
    "Instance segmentation goes a step further than object detection and involves identifying individual objects in an image and segmenting them from the rest of the image.\n",
    "\n",
    "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object. Instance segmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dec45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "# object classes\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # coordinates\n",
    "    for r in results:\n",
    "        masks = r.masks\n",
    "        boxes = r.boxes\n",
    "        if masks is None:  # Skip if no masks detected\n",
    "            continue\n",
    "\n",
    "        for mask, box in zip(masks, boxes):\n",
    "            mask = mask.xy[0].astype(int)\n",
    "\n",
    "            cls = int(box.cls[0])\n",
    "            # Draw filled mask overlay\n",
    "            if cls == 0:\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (255, 0, 0)\n",
    "            overlay = img.copy()\n",
    "            cv2.fillPoly(overlay, [mask], color=color)  # green mask\n",
    "            img = cv2.addWeighted(overlay, 0.5, img, 0.5, 0)\n",
    "\n",
    "            # object details\n",
    "            org = [int(box.xyxy[0][0]), int(box.xyxy[0][1])]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90bf8",
   "metadata": {},
   "source": [
    "## Pose Estimation\n",
    "Pose estimation is a task that involves identifying the location of specific points in an image, usually referred to as keypoints. The keypoints can represent various parts of the object such as joints, landmarks, or other distinctive features. The locations of the keypoints are usually represented as a set of 2D [x, y] or 3D [x, y, visible] coordinates.\n",
    "\n",
    "The output of a pose estimation model is a set of points that represent the keypoints on an object in the image, usually along with the confidence scores for each point. Pose estimation is a good choice when you need to identify specific parts of an object in a scene, and their location in relation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# model\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    for r in results:\n",
    "        if r.keypoints is None:  # skip if no keypoints detected\n",
    "            continue\n",
    "    \n",
    "        kps = r.keypoints.xy.cpu().numpy()  # shape: (num_objects, num_keypoints, 2)\n",
    "        \n",
    "        for obj_kps in kps:\n",
    "            for x, y in obj_kps:\n",
    "                if x > 0 and y > 0:  # valid point\n",
    "                    cv2.circle(img, (int(x), int(y)), 4, (0, 255, 0), -1)\n",
    "        \n",
    "        skeleton = [\n",
    "            (5, 7), (7, 9),    # left arm\n",
    "            (6, 8), (8, 10),   # right arm\n",
    "            (11, 13), (13, 15),# left leg\n",
    "            (12, 14), (14, 16),# right leg\n",
    "            (5, 6), (11, 12),  # shoulders and hips\n",
    "            (5, 11), (6, 12)   # torso\n",
    "        ]\n",
    "\n",
    "        for obj_kps in kps:\n",
    "            for (i, j) in skeleton:\n",
    "                x1, y1 = obj_kps[i]\n",
    "                x2, y2 = obj_kps[j]\n",
    "                if x1 > 0 and y1 > 0 and x2 > 0 and y2 > 0:\n",
    "                    cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "\n",
    "        # Show image with pose estimation\n",
    "    cv2.imshow(\"YOLO Pose Estimation\", img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf39cb",
   "metadata": {},
   "source": [
    "## Extras: How to train your own YOLO model\n",
    "If you want to train your own YOLO model, you can follow these general steps:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model with MPS\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list contains map50-95 of each category\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model([\"image1.jpg\", \"image2.jpg\"])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk\n",
    "\n",
    "\n",
    "# Export the model to another format\n",
    "model.export(format=\"onnx\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
